{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_HorseHuman.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JsNUZyyxMz_Z",
        "outputId": "8ea5ac3c-faea-4384-cfe1-0117e754d1c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-05 07:51:16--  http://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.202.128, 173.194.193.128, 173.194.197.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.202.128|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 149574867 (143M) [application/zip]\n",
            "Saving to: ‘horse-or-human.zip’\n",
            "\n",
            "horse-or-human.zip  100%[===================>] 142.65M   156MB/s    in 0.9s    \n",
            "\n",
            "2022-05-05 07:51:17 (156 MB/s) - ‘horse-or-human.zip’ saved [149574867/149574867]\n",
            "\n",
            "--2022-05-05 07:51:17--  http://storage.googleapis.com/laurencemoroney-blog.appspot.com/validation-horse-or-human.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.214.128, 108.177.111.128, 142.250.128.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.214.128|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11480187 (11M) [application/zip]\n",
            "Saving to: ‘validation-horse-or-human.zip’\n",
            "\n",
            "validation-horse-or 100%[===================>]  10.95M  71.2MB/s    in 0.2s    \n",
            "\n",
            "2022-05-05 07:51:18 (71.2 MB/s) - ‘validation-horse-or-human.zip’ saved [11480187/11480187]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# HorseVsPeople\n",
        "\n",
        "\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# http://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip unzip and add to \"./train_data\"\n",
        "# http://storage.googleapis.com/laurencemoroney-blog.appspot.com/validation-horse-or-human.zip unzip and add to \"./val_data\"\n",
        "\n",
        "!wget http://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip\n",
        "!wget http://storage.googleapis.com/laurencemoroney-blog.appspot.com/validation-horse-or-human.zip\n",
        "#\n",
        "TRAIN_DIR = \"./train_data\"\n",
        "VAL_DIR = \"./val_data\"\n",
        "!unzip -q horse-or-human.zip -d $TRAIN_DIR\n",
        "!unzip -q validation-horse-or-human.zip -d $VAL_DIR\n",
        "\n",
        "TRAIN_DIR = \"./train_data\"\n",
        "VAL_DIR = \"./val_data\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_sample_fn = os.listdir(os.path.join(TRAIN_DIR, \"horses\"))[0]\n",
        "train_sample_path = os.path.join(TRAIN_DIR, \"horses\", train_sample_fn)\n",
        "\n",
        "img = Image.open(train_sample_path)\n",
        "img_data = np.asarray(img)\n",
        "\n",
        "print(\"Image shape:\", img_data.shape)\n",
        "print(\"Pixel in range:\", np.min(img_data), np.max(img_data))\n",
        "\n",
        "print(img_data[:1])\n",
        "\n",
        "\n",
        "img_size = (150, 150)\n",
        "img_shape = (*img_size, 3)    # We didn't include transparency channel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCP6q8CtM5U0",
        "outputId": "02387dca-cea7-4120-8442-bbdaea78a60e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image shape: (300, 300, 4)\n",
            "Pixel in range: 1 255\n",
            "[[[ 24  30  28 255]\n",
            "  [ 28  36  37 255]\n",
            "  [ 34  48  56 255]\n",
            "  ...\n",
            "  [ 87  77  72 255]\n",
            "  [ 74  70  66 255]\n",
            "  [ 50  57  64 255]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "def gen_new_data(data_folder):\n",
        "  data_gen = ImageDataGenerator(rescale=1./255)\n",
        "  data = data_gen.flow_from_directory(\n",
        "      data_folder,\n",
        "      target_size=img_size,\n",
        "      batch_size=64,\n",
        "      class_mode=\"binary\",\n",
        "  )\n",
        "  return data\n",
        "\n",
        "train_data_gen = gen_new_data(TRAIN_DIR)\n",
        "val_data_gen = gen_new_data(VAL_DIR)\n",
        "\n",
        "\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout, BatchNormalization\n",
        "from tensorflow.keras.layers.experimental.preprocessing import Rescaling"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMqiPvuNNFCM",
        "outputId": "d3e2998b-7c35-4058-c099-7c54db4a21ad"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1027 images belonging to 2 classes.\n",
            "Found 256 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "model_2 = Sequential([\n",
        "  Conv2D(16, 3, activation='elu', input_shape=(150, 150, 3)),\n",
        "  BatchNormalization(),\n",
        "  MaxPooling2D(),\n",
        "  Conv2D(32, 3, activation='elu'),\n",
        "  MaxPooling2D(),\n",
        "  tf.keras.layers.SpatialDropout2D(0.6),\n",
        "  Conv2D(64, 3, activation='elu'),\n",
        "  MaxPooling2D(),\n",
        "  Conv2D(128, 3, activation='elu'),\n",
        "  MaxPooling2D(),\n",
        "  Flatten(),\n",
        "  Dense(512, activation='elu'),\n",
        "  Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model_2.compile(loss='binary_crossentropy',\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "#callback=tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5) , callbacks=[callback]\n",
        "\n",
        "# Fit the model\n",
        "model_2.fit(train_data_gen, epochs=20, validation_data=val_data_gen)\n",
        "\n",
        "model_2.evaluate(val_data_gen)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmr6Jfo6NG6G",
        "outputId": "a08e4687-e2e2-4c7e-b261-2a9cc3b35be4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "17/17 [==============================] - 18s 452ms/step - loss: 1.7091 - accuracy: 0.7702 - val_loss: 1.1067 - val_accuracy: 0.7070\n",
            "Epoch 2/20\n",
            "17/17 [==============================] - 7s 439ms/step - loss: 0.1636 - accuracy: 0.9435 - val_loss: 0.1672 - val_accuracy: 0.9375\n",
            "Epoch 3/20\n",
            "17/17 [==============================] - 7s 438ms/step - loss: 0.1044 - accuracy: 0.9572 - val_loss: 0.1884 - val_accuracy: 0.9414\n",
            "Epoch 4/20\n",
            "17/17 [==============================] - 7s 430ms/step - loss: 0.0786 - accuracy: 0.9747 - val_loss: 0.3392 - val_accuracy: 0.8945\n",
            "Epoch 5/20\n",
            "17/17 [==============================] - 7s 431ms/step - loss: 0.0473 - accuracy: 0.9834 - val_loss: 0.1372 - val_accuracy: 0.9492\n",
            "Epoch 6/20\n",
            "17/17 [==============================] - 8s 442ms/step - loss: 0.0271 - accuracy: 0.9932 - val_loss: 0.1767 - val_accuracy: 0.9453\n",
            "Epoch 7/20\n",
            "17/17 [==============================] - 7s 419ms/step - loss: 0.0183 - accuracy: 0.9951 - val_loss: 0.2324 - val_accuracy: 0.9336\n",
            "Epoch 8/20\n",
            "17/17 [==============================] - 7s 417ms/step - loss: 0.0130 - accuracy: 0.9971 - val_loss: 0.3603 - val_accuracy: 0.8945\n",
            "Epoch 9/20\n",
            "17/17 [==============================] - 7s 422ms/step - loss: 0.0061 - accuracy: 0.9990 - val_loss: 0.4645 - val_accuracy: 0.8789\n",
            "Epoch 10/20\n",
            "17/17 [==============================] - 7s 434ms/step - loss: 0.0112 - accuracy: 0.9971 - val_loss: 0.4203 - val_accuracy: 0.9062\n",
            "Epoch 11/20\n",
            "17/17 [==============================] - 7s 432ms/step - loss: 0.0081 - accuracy: 0.9981 - val_loss: 0.9284 - val_accuracy: 0.8555\n",
            "Epoch 12/20\n",
            "17/17 [==============================] - 7s 431ms/step - loss: 0.0114 - accuracy: 0.9961 - val_loss: 0.6953 - val_accuracy: 0.8594\n",
            "Epoch 13/20\n",
            "17/17 [==============================] - 7s 431ms/step - loss: 0.0061 - accuracy: 0.9981 - val_loss: 0.9568 - val_accuracy: 0.8516\n",
            "Epoch 14/20\n",
            "17/17 [==============================] - 7s 431ms/step - loss: 0.0063 - accuracy: 0.9990 - val_loss: 0.9097 - val_accuracy: 0.8438\n",
            "Epoch 15/20\n",
            "17/17 [==============================] - 7s 429ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 1.0419 - val_accuracy: 0.8281\n",
            "Epoch 16/20\n",
            "17/17 [==============================] - 7s 430ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 1.4994 - val_accuracy: 0.8047\n",
            "Epoch 17/20\n",
            "17/17 [==============================] - 7s 424ms/step - loss: 0.0095 - accuracy: 0.9971 - val_loss: 1.6778 - val_accuracy: 0.8203\n",
            "Epoch 18/20\n",
            "17/17 [==============================] - 7s 420ms/step - loss: 0.0300 - accuracy: 0.9903 - val_loss: 0.7714 - val_accuracy: 0.8516\n",
            "Epoch 19/20\n",
            "17/17 [==============================] - 7s 442ms/step - loss: 0.0117 - accuracy: 0.9981 - val_loss: 1.0699 - val_accuracy: 0.8555\n",
            "Epoch 20/20\n",
            "17/17 [==============================] - 7s 426ms/step - loss: 0.0090 - accuracy: 0.9951 - val_loss: 0.6898 - val_accuracy: 0.8672\n",
            "4/4 [==============================] - 1s 220ms/step - loss: 0.6898 - accuracy: 0.8672\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6897506713867188, 0.8671875]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_2.save('e3_q2.h5')"
      ],
      "metadata": {
        "id": "bswBMaWdNKcd"
      },
      "execution_count": 5,
      "outputs": []
    }
  ]
}